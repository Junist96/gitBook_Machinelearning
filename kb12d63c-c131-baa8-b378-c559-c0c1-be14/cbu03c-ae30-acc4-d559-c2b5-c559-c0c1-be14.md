## 2. 앙상블 분류

혼성 모델 
- 앙상블 : 같은 문제에 서로 다른 알고리즘 적용 하여 문제 해결 
- 연산공유 : 하나의 해에 2개 이상의 알고리즘이 개입 (eg. 유전알고리즘 + 순차 탐색, 신경망 + 퍼지)

> 메타 휴리스틱 : 다른 알고리즘을 포함하고 있는 알고리즘 

### 2.2 개요
- 여러 분류 알고리즘 다 돌려보고 결과를 투표를 통해 선택 
    - 주어진 학습 데이터 집합에 대해서 여러 개의 서로 다른 분류기를 만들고, 
    - 이들 분류기의 판정 결과를 투표/가중치투표 방식으로 결합
- 데이터가 적거나 많거나 상관없이 동작, Cross Validation과 유사 


### 2.2 앙상블 분류기 방식
- 배깅 : 한번에 여러 분류기 생성, eg. RandomForest
- 부스팅: 순차적으로 여러 분류기 생성, eg.AdaBoost


#### A. 배깅(bagging, bootstrap aggregating)
- 붓스트랩을 통해 여러 개의 학습 데이터 집합을 만들고, 
- 각 학습 데이터집합별로 분류기를 만들어, 이들이 투표나 가중치 투표를 하여 최종판정을 하는 기법

> 각기 다른 분류기가 각기 다른 부분의 데이터 사용, 알고리즘이 다르다(다양성)

![](http://i.imgur.com/vcJPBbe.png)

|부트스트랩(cf. 복원추출)|
|-|
|- 주어진 학습 데이터 집합에서 `복원추출`하여,<br>  - 다수의 학습 데이터 집합을 만들어내는 기법|
|목적 : 똑같은 데이터 반복 사용 문제점 해결|
|효과: Overfitting방지|
|Bootstrap : 장화를 신다. 시작 단계 |


###### [예] 서브배깅(Subbagging)
- Subsample + bagging 
- 원본데이터의 1/2정도의 크기로 샘플링
- 결과는 전체를 샘플링 한것과 비슷 


###### [예] Random forest
- 분류기로 결정트리를 사용하는 배깅 기법
- 여러개의 트리를 만들어서 사용 

###### [예] Robust Bagging
- 최종 분류기 선택시 평균(=이상치 영향 받음)을 사용하지 않고 중앙 값을 사용하는 알고리즘 


#### B. 부스팅(boosting)
- k개의 분류기를 순차적으로 만들어 가는 앙상블 분류기 생성 방법
- 분류 정확도에 따라 학습 데이터에 **가중치**를 변경해가면서 분류기 생성
    - 순차적으로 분류기 생성, cf. 배깅은 한번에 만듬 
- 예측 성능이 떨어지는 규칙(weak/base algorithm)들을 조합하여 좀더 높은 성능을 발휘 하는 방법 

![](http://i.imgur.com/WBotdrm.png)

###### [예] Stumping
- 스텀핑 = 나무를 자를 떄 남는 작은 부분 의미 
- 트리에 적용되는 그한 형태의 부스팅 알고리즘 


###### [예] AdaBoost
- 프론더, 샤피어 제안(1996)
- 이학습자가 얼마나 어려워했는지에 따라서 데이터에 가중치 부여 전 
- 분류 규칙을 순차적으로 생성, 생성시 이전 분류 규칙에서 얻은 관측값에서 샘플 데이터의 분포를 재조정 
    - 이전 분류에서 오분류 데이터에는 높은 가중치 부여, 가중치$$=\alpha = \frac{1-\epsilon}{\epsilon}$$
    - 이전 분류에서 정분류 데이터에는 낮은 가중치 부여 
    - 목적: 분류하기 힘든 데이터에 우선권 부여 
    
#### C. Mixture Algorithm
- 학습을 통해 분류기를 합치는 알고리즘 
- 네트워크를 학습시키는 알반적인 방법 = EM 알고리즘 



