# 기계학습 

![](http://i.imgur.com/zIHT4N4.png)
[기계학습 2](http://www.kocw.net/home/cview.do?lid=79a36e94d86a2ddc)
- 결정트리 (0:00:00~0:49:00)
- 앙상블 (0:49:01 ~

## 1. 결정 트리 

|결정트리 문제 | 분할 척도 | 특징|
|-|-|-|
|분류|정보이득||
|분류|정보이득비||
|분류|지니 지수 ||
|회귀|표준편차축소(SDR)||

### 1.1 개요
트리 형태로 의사결정 지식을 표현한 것
- 내부 노드(internal node) : 비교 속성
- 간선(edge) : 속성 값
- 단말 노드(terminal node) : 부류(class), 대표값

### 1.2 알고리즘 

분할속성 결정시 `엔트로피` 이용 
- 동질적인 정보 측정 
    - 섞인 정도가 클수록 큰 값 
    - 원래는 정보량 측정 목적의 척도 
- 공식 : $$H(x) = - \sum_e p(c)log_2 P(c)$$
    - p(c) : 분류 c에 속하는 비율 
    - \- : log함수는 1이하일때 음수 값이므로 이를 막기 위해 (-)를 앞에 붙임??

![](http://i.imgur.com/1KqYn82.png)


![](http://i.imgur.com/YpWA8cL.png)

###### Step 1. 각 선택에 대한 엔트로피 계산
eg. 

Pattern : 사각형(9개) vs. 삼각형(5개)
- $$p(사각형) = \frac{9}{14} , p(삼각형) = \frac{5}{14}$$ 
- 엔트로피 $$I= -\sum_c p(c) log_2 p(c) \rightarrow -\frac{9}{14}log_2\frac{9}{14}-\frac{5}{14}log_2\frac{5}{14}=0.940$$

###### Step 2. 정보 이득 계산 

IG(pattern) = 0.246

###### Step 3. 각 분류 기준 별로 모두 수행 

IG(pattern) = 0.246
IG(Outline) = 0.151
IG(Dot) = 0.048

###### Step 4. 분할 속성 선택 

정보이득이 큰것 선택 : Pattern
 
### 1.3 단점 및 해결책 
단점: 분할속성 척도로 `정보이득(IG)`사용시 속성값이 많은것 선호 
    - 속성값이 많으면 데이터집합을 많은 부분집합으로 분할, 작은 부분집합은 동질적인 경향

해결책 : 개선된 척도 사용 
- 정보 이득비
- 지니 지수 

#### A. 개선된 척도 : 정보 이득비 
속성값이 많은 속성에 패널티줌 = `I(A)`로 나누어 줌 

$$GainRation(A) = \frac{IG(A)}{I(A)} = \frac{I-I_{res}(A)}{I(A)}$$

패널 I(A) = $$-\sum_v p(v)log_2(p(v))$$
- 속성 A의 속성값을 부류로 간주 하여 계산된 엔트로피
- 속성값이 많을수록 커지는 경향 

![](http://i.imgur.com/TGfBsZ4.png)

#### B. 지니 지수(Gini Index)

###### Step 1. 지니값 계산 
두 확률의 곲의 총합으로 계산 : Gini = $$\sum p(i)p(j)$$

예시] Pattern : 사각형(9개) vs. 삼각형(5개)
$$p(사각형) = \frac{9}{14} , p(삼각형) = \frac{5}{14} \rightarrow Gini = \frac{9}{14} \times \frac{5}{14} = 0.230 $$ 

###### Step 2. 속성 A에 대한 지니 지수값 가중 평균 
Gini(A) = $$\sum_v p(v) \sum_{i \neq j}p(i|v)p(j|v)$$

###### Step 3. 지니 지수 이득

GiniGaim(A) = Gini - Gini(A)

![](http://i.imgur.com/1q6JuDB.png)

### 1.4 다양한 결정 트리 알고리즘 
ID3 알고리즘
- 범주형(categorical) 속성값을 갖는 데이터에 대한 결정트리 학습
- 예. PlayTennis, 삼각형/사각형 문제

C4.5 알고리즘
- 범주형 속성값과 수치형 속성값을 갖는 데이터로 부터 결정트리 학습
- ID3를 개선한 알고리즘

C5.0 알고리즘
- C4.5를 개선한 알고리즘

CART 알고리즘
- 수치형 속성을 갖는 데이터에 대해 적용


### 1.5 결정트리를 이용한 회귀분석
차이점 
- 단말 노드가 분류가 아닌 수치값

분할 척도 선택 
- 표준편차 축소(Reduction of standard deviation,SDR)을 최대로 하는 속성 선택 
- SDR = SD-SD(A)
    - SD = 표준편차
    - SD(A) = 속성 A를 기준으로 분할 후의 부분 집합별 표준표차의 가중 평균
    
![](http://i.imgur.com/byvMwpy.png)

## 2. 앙상블 분류

### 2.2 개요
- 여러 분류 알고리즘 다 돌려보고 결과를 투표를 통해 선택 
    - 주어진 학습 데이터 집합에 대해서 여러 개의 서로 다른 분류기를 만들고, 
    - 이들 분류기의 판정 결과를 투표/가중치투표 방식으로 결합



### 2.2 앙상블 분류기 방식
- 배깅 : 한번에 여러 분류기 생성, eg. RandomForest
- 부스팅: 순차적으로 여러 분류기 생성, eg.AdaBoost

#### A. 배깅(bagging, bootstrap aggregating)
- 붓스트랩을 통해 여러 개의 학습 데이터 집합을 만들고, 
- 각 학습 데이터집합별로 분류기를 만들어, 이들이 투표나 가중치 투표를 하여 최종판정을 하는 기법

> 데이터만 다르고, 알고리즘은 같은거??

![](http://i.imgur.com/vcJPBbe.png)

|부트스트랩(cf. 복원추출)|
|-|
|- 주어진 학습 데이터 집합에서 `복원추출`하여,<br>  - 다수의 학습 데이터 집합을 만들어내는 기법|
|목적 : 똑같은 데이터 반복 사용 문제점 해결|
|Bootstrap : 장화를 신다. 시작 단계 |


###### [예] Random forest
- 분류기로 결정트리를 사용하는 배깅 기법
- 여러개의 트리를 만들어서 사용 

#### B. 부스팅(boosting)
- k개의 분류기를 순차적으로 만들어 가는 앙상블 분류기 생성 방법
- 분류 정확도에 따라 학습 데이터에 **가중치**를 변경해가면서 분류기 생성
    - 순차적으로 분류기 생성, cf. 배깅은 한번에 만듬 

![](http://i.imgur.com/WBotdrm.png)

###### [예] AdaBoost

## 3. K-근접이웃 알고리즘 

