# 기계학습 

![](http://i.imgur.com/zIHT4N4.png)
- 신경망/딥러닝/강화 학습은 별도 Chapter에서 정리

## 1. 결정 트리 

ID3 :  시드니대학교 퀸란(Quinlan), `엔트로피`+`정보이득` 개념 활요한 결정트리 알고리즘 중 하나
- C4.5, C5.0, CART(정보이득 대신 지니지수 사용)로 발전 
- 배깅, 부스팅등 보완적 방법 연동 

|결정트리 문제 | 분할 척도 | 특징|
|-|-|-|
|분류|정보이득||
|분류|정보이득비||
|분류|지니 지수 ||
|회귀|표준편차축소(SDR)||

### 1.1 개요
트리 형태로 의사결정 지식을 표현한 것
- 내부 노드(internal node) : 비교 속성
- 간선(edge) : 속성 값
- 단말 노드(terminal node) : 부류(class), 대표값

### 1.2 알고리즘 

분할속성 결정시 `엔트로피` + `정보이득` 이용 

#### A. 엔트로피 
- 불확실성을 계량화 하기 위한 수학적 도구 : 엔트로피가 낮을수록 확실성 증가 
- 동질적인 정보 측정 
    - 섞인 정도가 클수록 큰 값 
    - 원래는 정보량 측정 목적의 척도 
- 공식 : $$H(x) = - \sum p(c)log_2 P(c)$$
    - p(c) : 분류 c에 속하는 비율 
    - \- : log함수는 1이하일때 음수 값이므로 이를 막기 위해 (-)를 앞에 붙임??

#### B. 정보이득 
- 데이터 분류하는 데 해당 속성 A의 효율성을 계량화하기 위한 척도
- 속성 A를 앎으로써 얻는 엔트로피의 감소 정도 
- 정보 이득이 클수록 확실성이 증가 

- $$ Gain(D,A) \equiv Entropy(D) - \sum_{v \in Values(A)}\{\frac{|D_v|}{|D|}Entropy(D_v)|\} $$
    - G(D,A) : 속성 A의 값을 앎으로써 발생하는 엔트로피의 감소 기대
    
    - $$values(A) : 속성 A가 취할 수 있는 가능한 모든 값들의 집합$$
    
    - $$D_v$$ : D의 부분집합으로 속석 A는 값 v를 갖는다. $$D_v = \{d \in D | A(d)=v\}$$
    


![](http://i.imgur.com/1KqYn82.png)


![](http://i.imgur.com/YpWA8cL.png)

###### Step 1. 각 선택에 대한 엔트로피 계산
eg. 

Pattern : 사각형(9개) vs. 삼각형(5개)
- $$p(사각형) = \frac{9}{14} , p(삼각형) = \frac{5}{14}$$ 
- 엔트로피 $$I= -\sum_c p(c) log_2 p(c) \rightarrow -\frac{9}{14}log_2\frac{9}{14}-\frac{5}{14}log_2\frac{5}{14}=0.940$$

###### Step 2. 정보 이득 계산 

IG(pattern) = 0.246

###### Step 3. 각 분류 기준 별로 모두 수행 

IG(pattern) = 0.246
IG(Outline) = 0.151
IG(Dot) = 0.048

###### Step 4. 분할 속성 선택 

정보이득이 큰것 선택 : Pattern
 
### 1.3 단점 및 해결책 
단점: 분할속성 척도로 `정보이득(IG)`사용시 속성값이 많은것 선호 
    - 속성값이 많으면 데이터집합을 많은 부분집합으로 분할, 작은 부분집합은 동질적인 경향

해결책 : 개선된 척도 사용 
- 정보 이득비
- 지니 지수 

#### A. 개선된 척도 : 정보 이득비 
속성값이 많은 속성에 패널티줌 = `I(A)`로 나누어 줌 

$$GainRation(A) = \frac{IG(A)}{I(A)} = \frac{I-I_{res}(A)}{I(A)}$$

패널 I(A) = $$-\sum_v p(v)log_2(p(v))$$
- 속성 A의 속성값을 부류로 간주 하여 계산된 엔트로피
- 속성값이 많을수록 커지는 경향 

![](http://i.imgur.com/TGfBsZ4.png)

#### B. 지니 지수(Gini Index)

###### Step 1. 지니값 계산 
두 확률의 곲의 총합으로 계산 : Gini = $$\sum p(i)p(j)$$

예시] Pattern : 사각형(9개) vs. 삼각형(5개)
$$p(사각형) = \frac{9}{14} , p(삼각형) = \frac{5}{14} \rightarrow Gini = \frac{9}{14} \times \frac{5}{14} = 0.230 $$ 

###### Step 2. 속성 A에 대한 지니 지수값 가중 평균 
Gini(A) = $$\sum_v p(v) \sum_{i \neq j}p(i|v)p(j|v)$$

###### Step 3. 지니 지수 이득

GiniGaim(A) = Gini - Gini(A)

![](http://i.imgur.com/1q6JuDB.png)

### 1.4 다양한 결정 트리 알고리즘 
ID3 알고리즘
- 범주형(categorical) 속성값을 갖는 데이터에 대한 결정트리 학습
- 예. PlayTennis, 삼각형/사각형 문제

C4.5 알고리즘
- 범주형 속성값과 수치형 속성값을 갖는 데이터로 부터 결정트리 학습
- ID3를 개선한 알고리즘

C5.0 알고리즘
- C4.5를 개선한 알고리즘

CART 알고리즘
- 수치형 속성을 갖는 데이터에 대해 적용


### 1.5 결정트리를 이용한 회귀분석
차이점 
- 단말 노드가 분류가 아닌 수치값

분할 척도 선택 
- 표준편차 축소(Reduction of standard deviation,SDR)을 최대로 하는 속성 선택 
- SDR = SD-SD(A)
    - SD = 표준편차
    - SD(A) = 속성 A를 기준으로 분할 후의 부분 집합별 표준표차의 가중 평균
    
![](http://i.imgur.com/byvMwpy.png)

## 2. 앙상블 분류

### 2.2 개요
- 여러 분류 알고리즘 다 돌려보고 결과를 투표를 통해 선택 
    - 주어진 학습 데이터 집합에 대해서 여러 개의 서로 다른 분류기를 만들고, 
    - 이들 분류기의 판정 결과를 투표/가중치투표 방식으로 결합



### 2.2 앙상블 분류기 방식
- 배깅 : 한번에 여러 분류기 생성, eg. RandomForest
- 부스팅: 순차적으로 여러 분류기 생성, eg.AdaBoost

#### A. 배깅(bagging, bootstrap aggregating)
- 붓스트랩을 통해 여러 개의 학습 데이터 집합을 만들고, 
- 각 학습 데이터집합별로 분류기를 만들어, 이들이 투표나 가중치 투표를 하여 최종판정을 하는 기법

> 데이터만 다르고, 알고리즘은 같은거??

![](http://i.imgur.com/vcJPBbe.png)

|부트스트랩(cf. 복원추출)|
|-|
|- 주어진 학습 데이터 집합에서 `복원추출`하여,<br>  - 다수의 학습 데이터 집합을 만들어내는 기법|
|목적 : 똑같은 데이터 반복 사용 문제점 해결|
|Bootstrap : 장화를 신다. 시작 단계 |


###### [예] Random forest
- 분류기로 결정트리를 사용하는 배깅 기법
- 여러개의 트리를 만들어서 사용 


#### B. 부스팅(boosting)
- k개의 분류기를 순차적으로 만들어 가는 앙상블 분류기 생성 방법
- 분류 정확도에 따라 학습 데이터에 **가중치**를 변경해가면서 분류기 생성
    - 순차적으로 분류기 생성, cf. 배깅은 한번에 만듬 
- 예측 성능이 떨어지는 규칙(weak/base algorithm)들을 조합하여 좀더 높은 성능을 발휘 하는 방법 

![](http://i.imgur.com/WBotdrm.png)

###### [예] AdaBoost
- 프론더, 샤피어 제안(1996)
- 분류 규칙을 순차적으로 생성, 생성시 이전 분류 규칙에서 얻은 관측값에서 갬플 데이터의 분포를 재조정 
    - 이전 분류에서 오분류 데이터에는 높은 가중치 부여 
    - 이전 분류에서 정분류 데이터에는 낮은 가중치 부여 
    - 목적: 분류하기 힘든 데이터에 우선권 부여 

## 3. K-근접이웃 알고리즘 

### 3.1 개요
- 초반에는 (입력, 결과)가 있는 데이터들을 저장하고 있음 
- 새로운 입력에 대한 결과를 추정할 때 결과를 아는 최근접한 k개의 데이터에 대한 결과정보를 이용하는 방법

단점
- 학습단계에서는 실질적인 학습이 일어나지 않고 데이터만 저장 후 새로운 데이터가 주어지면 저장된 데이터를 이용하여 학습 
    - 학습데이터가 크면 메모리 문제
    - 게으른 학습(lazy learning
    - 시간이 많이 걸릴 수 있음
    
### 3.2 알고리즘 

###### Step 1. 질의(query)와 데이터간의 거리 계산

수치 데이터
- 유클리디언 거리(Euclidian distance) 
- 응용분야의 특성에 맞춰 개발

범주형 데이터
- 응용분야의 특성에 맞춰 개발

###### Step 2. 효율적으로 근접이웃 탐색

- 색인(indexing) 자료구조 사용: R-트리, k-d 트리 등

- 문제점 : 데이터의 개수가 많아지면 계산시간 증가 문제

###### Step 3. 근접 이웃 k개로 부터 결과를 추정

- 분류
    - 다수결 투표(majority voting) : 개수가 많은 범주 선택

- 회귀분석
    - 평균 : 최근접 k개의 평균값
    - 가중합(weighted sum) : 거리에 반비례하는 가중치 사용

## 4. 단순 베이즈 분류기(Naive Bayes Classifier)
- 부류 결정 규칙을 `조건부 확률`로 결정 

![](http://i.imgur.com/HCjZq6k.png)

###### [예시]

![](http://i.imgur.com/NjYJQJF.png)

## 5. Support Vector Machine (SVM)
- Vladimir Vapnik이 제안
- 분류 오차를 줄이면서 동시에 여백을 최대로 하는 결정 경계(decision boundary)를 찾는 이진 분류기(binary classifier)

> 분류 문제를 해결 하는 방법 

![](http://i.imgur.com/uF7mAZO.png)

여백(margin): 결정 경계와 가장 가까이에 있는 학습 데이터까지의 거리

서포트 벡터(support vector): 결정 경계로부터 가장 가까이에 있는 학습 데이터들